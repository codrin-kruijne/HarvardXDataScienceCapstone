---
title: "Basic MovieLens recommendation script"
subtitle: "HarvardX Data Science Capstone Project"
author: "Codrin Kruijne"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_notebook: default
urlcolor: blue
---

```{r setup, message = FALSE, warning = FALSE}
# PLEASE NOTE
# THIS SCRIPT HAS ONLY BEEN TESTED ON A 6-CORE (12-THREAD), 64GB MEMORY MACHINE
# AND WILL PROBABLY NOT RUN WITH LOWER COMPUTER SPECIFICATIONS.
parallel::detectCores()
memory.limit()

# Script settings
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	cache = TRUE,
	tidy.opts = list(width.cutoff = 100),
	tidy = TRUE
)
script_start <- Sys.time()

# Load required packages
library(tidyverse)
library(lubridate)
library(parallel)
library(doParallel)
```

# Loading packages and data

The [dataset](http://files.grouplens.org/datasets/movielens/ml-10m.zip) was downloaded from the [GroupLens website](https://grouplens.org) using the course provided script and stored locally. Information about the data was found on the [MovieLens README] (http://files.grouplens.org/datasets/movielens/ml-10m-README.html)

```{r Data Exploration}
# Load locally stored data
edx_original <- readRDS("edx.RData")
print(str(edx_original))
validation_original <- readRDS("validation.RData")
print(str(validation_original))

# Make sure movies and users are in both train and test sets
train_data <- edx_original
print(str(train_data))
test_data <- validation_original %>% 
     semi_join(train_data, by = "movieId") %>%
     semi_join(train_data, by = "userId")
print(str(test_data))

all_data <- setNames(list(train_data, test_data), c("train_data", "test_data"))

# Let's have a quick look at the rating training data
rating_hist <- hist(train_data$rating)
saveRDS(rating_hist, "rating_hist.rds")
plot(rating_hist)
```

# Preparing data

```{r Data Preparation, include = FALSE, results = 'hide'}

# Let's prepare parallel processing
no_cores <- detectCores() - 4
pcl <- makeCluster(no_cores)
invisible(clusterEvalQ(pcl, library(dplyr)))
invisible(clusterEvalQ(pcl, library(lubridate)))
invisible(clusterEvalQ(pcl, library(stringr)))

# Some basic data transformation is done to both training and test data

# Separating user characteristics, ratings and movie features
extract_user_data <- function(original_data){
  original_data %>% select(userId) %>%
                    mutate(userId = as.factor(userId)) %>% unique()
}

# Separating and transforming rating data
extract_rating_data <- function(original_data){
  original_data %>% select(userId, movieId, rating, timestamp) %>%
                    mutate(userId = as.factor(userId),
                           movieId = as.factor(movieId),
                           date = as.POSIXct(timestamp, origin = "1970-01-01")) %>% # Turn rating timestamps to date
                           mutate(rating_year = year(date)) %>%
                    select(-timestamp, -date)
}

# Separating and transforming movie data
extract_movie_data <- function(original_data){

  # Let's determine which genres we have
  unique_genres <- unique(str_extract(original_data$genres, "[^\\|]+"))

  movie_data <- original_data %>% select(movieId, title, genres) %>%
                                  unique() %>%
                                  mutate(movieId = as.factor(movieId),
                                         movie_year = as.factor(str_sub(title, -5, -2)),
                                         title = str_sub(title, 0, -8))
  
  
  # Now lets add genres as columns
  movie_data[, unique_genres] <- NA
  
  # For each of the ratings we detect present genres
  for(g in 1:length(unique_genres)) {
    movie_data[, unique_genres[g]] <- str_detect(movie_data$genres, unique_genres[g])
  }
  # Drop old column
  movie_data <- movie_data %>% select(-genres)
  
}

# Apply all data transformations in parallel
user_data <- parLapply(pcl, all_data, extract_user_data)
rating_data <- parLapply(pcl, all_data, extract_rating_data)
movie_data <- parLapply(pcl, all_data, extract_movie_data)

# Let's explore our new data sets
str(user_data)
str(rating_data)
str(movie_data)
```

# Calculating statistics

```{r Data Statistics}
# For all data entities we calculate some basic statistics

# User statistics
calc_user_stats <- function(rating_data){
  
  rating_data %>% group_by(userId) %>%
                  summarise(user_ratings = n(),
                            user_median = median(rating),
                            user_mean = mean(rating),
                            user_sd = sd(rating)) %>%
                  select(userId, user_median, user_mean, user_sd) %>%
                  unique()

}

user_stats <- parLapply(pcl, rating_data, calc_user_stats)

# Movie rating statistics
calc_movie_stats <- function(rating_data){
  
  rating_data %>% group_by(movieId) %>%
                  summarise(movie_ratings = n(),
                            movie_median = median(rating),
                            movie_mean = mean(rating),
                            movie_sd = sd(rating)) %>%
                  select(movieId, movie_median, movie_mean, movie_sd) %>%
                  unique()
  
} 

movie_stats <- parLapply(pcl, rating_data, calc_movie_stats)

# Merge statistics back
merge_data <- list(list(rating_data[[1]], user_stats[[1]], movie_stats[[1]]), # train data
                   list(rating_data[[2]], user_stats[[2]], movie_stats[[2]])) # test data

merge_stats <- function(data_stats){
  
  rating_data <- data_stats[[1]]
  user_stats <- data_stats[[2]]
  movie_stats <- data_stats[[3]]
    
  rating_data %>% inner_join(user_stats, by = "userId") %>%
                  inner_join(movie_stats, by = "movieId")

}

rating_data <- parLapply(pcl, merge_data, merge_stats)

## Rating characteristics
calc_rating_stats <- function(rating_data){
  rating_mean <- mean(rating_data$rating)

  rating_data <- rating_data %>% mutate(rating_mean_diff = rating - rating_mean) %>% 
                                 mutate(movie_mean_diff = rating - movie_mean) %>% 
                                 mutate(user_mean_diff = rating - user_mean)
  
}

rating_data <- parLapply(pcl, rating_data, calc_rating_stats)
stopCluster(pcl)

```

# Derived models
```{r Derived Models}

# Let's take the rating data with the added statistics
train_ratings <- rating_data[[1]]
test_ratings <- rating_data[[2]]

saveRDS(train_ratings, "train_ratings.rds")
saveRDS(test_ratings, "test_ratings.rds")

# RSME loss function function to evaluate models as prescribed by assignment
RMSE <- function(predicted_ratings, true_ratings){
     sqrt(mean((true_ratings - predicted_ratings)^2))
}

# Deried models

# Naive model using average rating
train_mean <- mean(train_ratings$rating)
train_mean

naive_rmse <- RMSE(test_ratings$rating, train_mean)

derived_results <- tibble(method = "Average training rating", RMSE = naive_rmse)

# Model using fixed number
fixed_number <- rep(3.5, nrow(test_ratings))

fixed_rmse <- RMSE(test_ratings$rating, fixed_number)

derived_results <- bind_rows(derived_results,
                             tibble(method = "Fixed number 3.5",
                                    RMSE = fixed_rmse ))

# Derived predictions

movie_pred <- test_ratings %>% group_by(movieId) %>%
               # Rating effect (effect of rating scale used; looking at differences from rating mean)      
                               summarise(rating_mean_diff_avg = train_mean + mean(rating_mean_diff),
               # Movie effect (effect of general movie popularity; looking at differences between user rating and movie mean)
                                         movie_mean_diff_avg = train_mean + mean(movie_mean_diff),
               # User effect (effect of user rating behaviour; looking at differeces between rating and average user rating)
                                         user_mean_diff_avg = train_mean + mean(user_mean_diff),
               # User-movie effect (effect of user rating behaviour; looking at differeces between user rating and average movie rating)
                                         user_movie_mean_diff_avg = train_mean + mean(mean(movie_mean_diff), mean(user_mean_diff)))

der_pred <- test_ratings %>% inner_join(movie_pred, by = "movieId")

# Calculate RMSEs
rating_effect_rmse <- RMSE(test_ratings$rating,
                           der_pred$rating_mean_diff_avg)
movie_mean_rmse <- RMSE(test_ratings$rating,
                        test_ratings$movie_mean)
movie_effect_rmse <- RMSE(test_ratings$rating,
                          der_pred$movie_mean_diff_avg)
user_effect_rmse <- RMSE(test_ratings$rating,
                         der_pred$user_mean_diff_avg)
user_movie_effect_rmse <- RMSE(test_ratings$rating,
                               der_pred$user_movie_mean_diff_avg)

# Print RMSEs

derived_results <- bind_rows(derived_results,
                             tibble(method = "Rating effect",
                                    RMSE = rating_effect_rmse))
derived_results <- bind_rows(derived_results,
                             tibble(method = "Movie mean",
                                    RMSE = movie_mean_rmse))
derived_results <- bind_rows(derived_results,
                             tibble(method = "Movie effect",
                                    RMSE = movie_effect_rmse))
derived_results <- bind_rows(derived_results,
                             tibble(method = "User effect",
                                    RMSE = user_effect_rmse))
derived_results <- bind_rows(derived_results,
                             tibble(method = "User-movie effect",
                                    RMSE = user_movie_effect_rmse))

saveRDS(derived_results, "derived_results.rds")
print(derived_results %>% arrange(RMSE))

```


# Train linear models
```{r Linear Model Training}

# Load locally stored data
train_ratings <- readRDS("train_ratings.rds")
test_ratings <- readRDS("test_ratings.rds")

# Free up memory
rm(edx_original,
   validation_original,
   train_data,
   test_data,
   all_data,
   user_data,
   movie_data,
   rating_data,
   merge_data,
   der_pred)
invisible(gc()) # garbage collection

# Set up parallel processing
no_cores <- detectCores() - 2
tcl <- makePSOCKcluster(no_cores)
registerDoParallel(tcl)
invisible(clusterEvalQ(tcl, library(caret)))

print(paste("Memory size before garbage collection: ", memory.size()))
invisible(gc())
print(paste("Memory size after garbage collection: ", memory.size()))

# Define models
user_formulas <- c("rating ~ user_median",
                   "rating ~ user_median + user_mean",
                   "rating ~ user_median + user_mean + user_sd")

movie_formulas <- c("rating ~ movie_median",
                   "rating ~ movie_median + movie_mean",
                   "rating ~ movie_median + movie_mean + movie_sd")

combined_formulas <- c("rating ~ movie_mean + user_mean",
                       "rating ~ movie_median + user_median")

full_formulas <- c("rating ~ movie_median + movie_mean + user_median + user_mean",
                   "rating ~ movie_median + movie_mean + movie_sd + user_median + user_mean + user_sd")

# Model training
e <- simpleError("Catch error")

train_lm <- function(lm_formula, lm_data){
  
  print(paste("Model formula: ", lm_formula))
  
  print(system.time({
    model <- tryCatch({
               train(as.formula(lm_formula),
                     data = lm_data, method = "lm", 
                     na.action = na.omit) 
             }, error = function(e) e, finally = print("Made it!"))
  }))
  
  # Full train objects are GBs large; save only the final model coefficients
  model_coef <- model$finalModel$coefficients
  
  # How's our memory?
  print(paste("Memory size before garbage collection: ", memory.size()))
  gc()
  print(paste("Memory size after garbage collection: ", memory.size()))
  
  # Save model locally
  saveRDS(model_coef, paste0("models/", lm_formula, ".rds"))
  # Return results
  print(model_coef)
  model
}

# Train and save locally
system.time(user_fits <- parLapply(tcl, user_formulas, train_lm, lm_data = train_ratings))
#saveRDS(user_fits, "user_fits.rds")
rm(user_fits)

system.time(movie_fits <- parLapply(tcl, movie_formulas, train_lm, lm_data = train_ratings))
#saveRDS(movie_fits, "movie_fits.rds")
rm(movie_fits)

system.time(combined_fits <- parLapply(tcl, combined_formulas, train_lm, lm_data = train_ratings))
#saveRDS(combined_fits, "combined_fits.rds")
rm(combined_fits)

system.time(full_fits <- parLapply(tcl, full_formulas, train_lm, lm_data = train_ratings))
#saveRDS(full_fits, "full_fits.rds")
rm(full_fits)

# Stop parallel processing
stopImplicitCluster()

# Clean up before next step
invisible(gc())

```

# Final model predictions and results
```{r Linear Model Results}

# Read model coeffecients and calculate RMSE

calculate_results <- function(model_formulas, test_ratings){
  str(test_ratings)
  
  for(m in 1:length(model_formulas)){
    
    coefs <- readRDS(paste0("models/", model_formulas[[m]], ".rds"))
    
    prediction <- rep(coefs[[1]], nrow(test_ratings)) # start with the intercept
    
    for(c in 2:length(coefs)){ # then go through each coefficient
      
      ce <- rep(coefs[[c]], nrow(test_ratings))
      
      prediction <- prediction + (ce * test_ratings[, names(coefs[c])]) # and add coef * value to prediction
      
    }
    
    RMSE <- function(predicted_ratings, true_ratings){
      
      rmse <- sqrt(mean((true_ratings - predicted_ratings)^2, na.rm = TRUE))
      
      rmse
    }

    model_rmse <- RMSE(prediction, test_ratings$rating)
    #print(paste("Model RMSE: ", model_rmse))
    
    #print(paste(model_formulas[[m]], model_rmse))
    
    results <- data.frame(model = model_formulas[m], rmse = model_rmse)
  }
  
  results
}

user_results <- lapply(user_formulas, calculate_results, test_ratings = test_ratings)
movie_results <- lapply(movie_formulas, calculate_results, test_ratings = test_ratings)
combined_results <- lapply(combined_formulas, calculate_results, test_ratings = test_ratings)
full_results <- lapply(full_formulas, calculate_results, test_ratings = test_ratings)

# Save results locally and print
saveRDS(user_results, "user_results.rds")
saveRDS(movie_results, "movie_results.rds")
saveRDS(combined_results, "combined_results.rds")
saveRDS(full_results, "full_results.rds")

# Lets have a look
lm_results <- bind_rows(c(user_results, movie_results, combined_results, full_results))
lm_results %>% arrange(rmse)
saveRDS(lm_results, "lm_results.rds")

# Clean up before next step
invisible(gc())

```

# Automatic Machine Learning with H2O.ai
```{r autoML}

# Shall we explore auto ML?

library(h2o)

h2o.init()
h2o.no_progress()

# Import train/test set into H2O
train <- as.h2o(readRDS("edx.RData"))
test <- as.h2o(readRDS("validation.RData"))

# Identify predictors and response
y <- "rating"
x <- setdiff(names(train), y)

# Run AutoML for 10 base models (limited to 1 hour max runtime by default)
print(Sys.time())
aml <- h2o.automl(x = x, y = y,
                  training_frame = train,
                  validation_frame = test,
                  max_models = 10,
                  seed = 1,
                  stopping_metric = "RMSE",
                  sort_metric = "RMSE")

# View the AutoML Leaderboard
lb <- aml@leaderboard
autoML_results <- as.data.frame(lb) %>% select(model_id, rmse)

saveRDS(autoML_results, "autoML_results.rds")
print(autoML_results)
      
```

```{r}
# How long did the whole script take?
script_end <- Sys.time()

print(paste("Total script running time: ", round(difftime(script_end, script_start, units = "mins"), 1), " minutes"))
```

# Extended automatic Machine Learning with H2O.ai
```{r Extended autoML, eval = FALSE}

# What will 10 hours of computation yield?

library(h2o)

h2o.init()
h2o.show_progress()

# Import train/test set into H2O
train <- as.h2o(readRDS("edx.RData"))
test <- as.h2o(readRDS("validation.RData"))

# Identify predictors and response
y <- "rating"
x <- setdiff(names(train), y)

# Run AutoML for 25 models (limited to 10 hour max runtime)
print(Sys.time())
xaml <- h2o.automl(x = x, y = y,
                  training_frame = train,
                  validation_frame = test,
                  max_models = 25,
                  seed = 1,
                  stopping_metric = "RMSE",
                  sort_metric = "RMSE",
                  max_runtime_secs = 36000)

# View the AutoML Leaderboard
xlb <- xaml@leaderboard
xautoML_results <- as.data.frame(xlb) %>% select(model_id, rmse)

saveRDS(xautoML_results, "xautoML_results.rds")
print(xautoML_results)
      
```